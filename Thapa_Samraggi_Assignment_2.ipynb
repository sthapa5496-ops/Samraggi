{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sthapa5496-ops/Samraggi/blob/main/Thapa_Samraggi_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "910c6837-c62d-4b02-c919-0f32b8ad8970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Error 429, retrying...\n",
            " Error 429, retrying...\n",
            " Error 429, retrying...\n",
            " Error 429, retrying...\n",
            " Error 429, retrying...\n",
            "‚úÖ Test complete: 1000 papers saved to batch9.csv\n",
            "‚úÖ Combined into papers_10000.csv with 10000 rows\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8802c5c1-bc16-4d42-abd8-a8d2a94592fd\", \"papers_10000.csv\", 6488167)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# (Step 1) Define query and output file\n",
        "search_term = \"machine learning\"   # you can change to \"data science\", \"AI\", etc.\n",
        "records_needed = 1000            # small batch just for testing\n",
        "batch_limit = 100                   # fetch 25 per request\n",
        "file_name = \"batch9.csv\"\n",
        "\n",
        "# (Step 2) API endpoint and fields to retrieve\n",
        "api_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "fields_required = \"title,abstract,year,authors\"\n",
        "\n",
        "# (Step 3) Function to fetch papers\n",
        "def fetch_papers(query, total, per_request):\n",
        "    results = []\n",
        "    offset = 0\n",
        "\n",
        "    while len(results) < total:\n",
        "        params = {\n",
        "            \"query\": query,\n",
        "            \"offset\": offset,\n",
        "            \"limit\": per_request,\n",
        "            \"fields\": fields_required\n",
        "        }\n",
        "        response = requests.get(api_url, params=params)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\" Error {response.status_code}, retrying...\")\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "\n",
        "        data = response.json().get(\"data\", [])\n",
        "        if not data:\n",
        "            break\n",
        "\n",
        "        results.extend(data)\n",
        "        offset += per_request\n",
        "        time.sleep(0.2)  # respect API limits\n",
        "\n",
        "    return results[:total]\n",
        "\n",
        "# (Step 4) Save to CSV\n",
        "def save_csv(records, filename):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Paper_Title\", \"Abstract\", \"Year\", \"Authors\"])\n",
        "        for paper in records:\n",
        "            title = paper.get(\"title\", \"\")\n",
        "            abstract = paper.get(\"abstract\", \"\")\n",
        "            year = paper.get(\"year\", \"\")\n",
        "            authors = \", \".join([a.get(\"name\", \"\") for a in paper.get(\"authors\", [])])\n",
        "            writer.writerow([title, abstract, year, authors])\n",
        "\n",
        "# (Step 5) Run test\n",
        "if __name__ == \"__main__\":\n",
        "    papers = fetch_papers(search_term, records_needed, batch_limit)\n",
        "    save_csv(papers, file_name)\n",
        "    print(f\"‚úÖ Test complete: {len(papers)} papers saved to {file_name}\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# list all your batch files\n",
        "batch_files = [f\"batch{i}.csv\" for i in range(1, 11)]\n",
        "\n",
        "# read and merge\n",
        "merged = pd.concat([pd.read_csv(f) for f in batch_files], ignore_index=True)\n",
        "\n",
        "# save as one big CSV\n",
        "merged.to_csv(\"papers_10000.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Combined into papers_10000.csv with\", len(merged), \"rows\")\n",
        "from google.colab import files\n",
        "files.download(\"papers_10000.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c7b92366-d164-43bd-d979-149f3b3f0c74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Removed noise\n",
            "                                            Abstract  \\\n",
            "0  We present Fashion-MNIST, a new dataset compri...   \n",
            "1                                                NaN   \n",
            "2  TensorFlow is an interface for expressing mach...   \n",
            "3  With the widespread use of artificial intellig...   \n",
            "4                                                NaN   \n",
            "\n",
            "                                         clean_noise  \n",
            "0  We present FashionMNIST a new dataset comprisi...  \n",
            "1                                                nan  \n",
            "2  TensorFlow is an interface for expressing mach...  \n",
            "3  With the widespread use of artificial intellig...  \n",
            "4                                                nan  \n",
            "‚úÖ Removed numbers\n",
            "                                         clean_noise  \\\n",
            "0  We present FashionMNIST a new dataset comprisi...   \n",
            "1                                                nan   \n",
            "2  TensorFlow is an interface for expressing mach...   \n",
            "3  With the widespread use of artificial intellig...   \n",
            "4                                                nan   \n",
            "\n",
            "                                    clean_no_numbers  \n",
            "0  We present FashionMNIST a new dataset comprisi...  \n",
            "1                                                nan  \n",
            "2  TensorFlow is an interface for expressing mach...  \n",
            "3  With the widespread use of artificial intellig...  \n",
            "4                                                nan  \n",
            "‚úÖ Removed stopwords\n",
            "                                    clean_no_numbers  \\\n",
            "0  We present FashionMNIST a new dataset comprisi...   \n",
            "1                                                nan   \n",
            "2  TensorFlow is an interface for expressing mach...   \n",
            "3  With the widespread use of artificial intellig...   \n",
            "4                                                nan   \n",
            "\n",
            "                                  clean_no_stopwords  \n",
            "0  present FashionMNIST new dataset comprising x ...  \n",
            "1                                                nan  \n",
            "2  TensorFlow interface expressing machine learni...  \n",
            "3  widespread use artificial intelligence AI syst...  \n",
            "4                                                nan  \n",
            "‚úÖ Lowercased text\n",
            "                                  clean_no_stopwords  \\\n",
            "0  present FashionMNIST new dataset comprising x ...   \n",
            "1                                                nan   \n",
            "2  TensorFlow interface expressing machine learni...   \n",
            "3  widespread use artificial intelligence AI syst...   \n",
            "4                                                nan   \n",
            "\n",
            "                                         clean_lower  \n",
            "0  present fashionmnist new dataset comprising x ...  \n",
            "1                                                nan  \n",
            "2  tensorflow interface expressing machine learni...  \n",
            "3  widespread use artificial intelligence ai syst...  \n",
            "4                                                nan  \n",
            "‚úÖ Applied stemming\n",
            "                                         clean_lower  \\\n",
            "0  present fashionmnist new dataset comprising x ...   \n",
            "1                                                nan   \n",
            "2  tensorflow interface expressing machine learni...   \n",
            "3  widespread use artificial intelligence ai syst...   \n",
            "4                                                nan   \n",
            "\n",
            "                                       clean_stemmed  \n",
            "0  present fashionmnist new dataset compris x gra...  \n",
            "1                                                nan  \n",
            "2  tensorflow interfac express machin learn algor...  \n",
            "3  widespread use artifici intellig ai system app...  \n",
            "4                                                nan  \n",
            "‚úÖ Applied lemmatization\n",
            "                                         clean_lower  \\\n",
            "0  present fashionmnist new dataset comprising x ...   \n",
            "1                                                nan   \n",
            "2  tensorflow interface expressing machine learni...   \n",
            "3  widespread use artificial intelligence ai syst...   \n",
            "4                                                nan   \n",
            "\n",
            "                                    clean_lemmatized  \n",
            "0  present fashionmnist new dataset comprising x ...  \n",
            "1                                                nan  \n",
            "2  tensorflow interface expressing machine learni...  \n",
            "3  widespread use artificial intelligence ai syst...  \n",
            "4                                                nan  \n",
            "\n",
            "üéâ Cleaning complete! File saved as papers_10000_cleaned.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_49bdcdec-2bbe-4ba0-8b69-b43343deb444\", \"papers_10000_cleaned.csv\", 31894328)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#  Load dataset\n",
        "df = pd.read_csv(\"papers_10000.csv\")\n",
        "\n",
        "# Let's assume the column to clean is \"Abstract\"\n",
        "text_col = \"Abstract\"\n",
        "\n",
        "# 1. Remove noise (special characters, punctuation)\n",
        "df[\"clean_noise\"] = df[text_col].astype(str).apply(lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x))\n",
        "print(\"‚úÖ Removed noise\")\n",
        "print(df[[\"Abstract\", \"clean_noise\"]].head())\n",
        "\n",
        "# 2. Remove numbers\n",
        "df[\"clean_no_numbers\"] = df[\"clean_noise\"].apply(lambda x: re.sub(r\"\\d+\", \"\", x))\n",
        "print(\"‚úÖ Removed numbers\")\n",
        "print(df[[\"clean_noise\", \"clean_no_numbers\"]].head())\n",
        "\n",
        "# 3. Remove stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "df[\"clean_no_stopwords\"] = df[\"clean_no_numbers\"].apply(\n",
        "    lambda x: \" \".join([word for word in x.split() if word.lower() not in stop_words])\n",
        ")\n",
        "print(\"‚úÖ Removed stopwords\")\n",
        "print(df[[\"clean_no_numbers\", \"clean_no_stopwords\"]].head())\n",
        "\n",
        "# 4. Lowercase all text\n",
        "df[\"clean_lower\"] = df[\"clean_no_stopwords\"].str.lower()\n",
        "print(\"‚úÖ Lowercased text\")\n",
        "print(df[[\"clean_no_stopwords\", \"clean_lower\"]].head())\n",
        "\n",
        "# 5. Stemming\n",
        "stemmer = PorterStemmer()\n",
        "df[\"clean_stemmed\"] = df[\"clean_lower\"].apply(\n",
        "    lambda x: \" \".join([stemmer.stem(word) for word in x.split()])\n",
        ")\n",
        "print(\"‚úÖ Applied stemming\")\n",
        "print(df[[\"clean_lower\", \"clean_stemmed\"]].head())\n",
        "\n",
        "# 6. Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df[\"clean_lemmatized\"] = df[\"clean_lower\"].apply(\n",
        "    lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()])\n",
        ")\n",
        "print(\"‚úÖ Applied lemmatization\")\n",
        "print(df[[\"clean_lower\", \"clean_lemmatized\"]].head())\n",
        "\n",
        "# (Final Step) Save new CSV with cleaned columns\n",
        "df.to_csv(\"papers_10000_cleaned.csv\", index=False)\n",
        "print(\"\\nüéâ Cleaning complete! File saved as papers_10000_cleaned.csv\")\n",
        "from google.colab import files\n",
        "files.download(\"papers_10000_cleaned.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "916535dd-6d4c-414c-a20b-a6e960e9bfcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "‚úÖ POS Tagging Counts:\n",
            "Nouns: 235\n",
            "Verbs: 88\n",
            "Adjectives: 59\n",
            "Adverbs: 11\n",
            "\n",
            "‚úÖ Dependency Parsing (word ‚Üí head relation):\n",
            "present --> amod --> comprising\n",
            "fashionmnist --> amod --> comprising\n",
            "new --> amod --> comprising\n",
            "dataset --> npadvmod --> comprising\n",
            "comprising --> nsubj --> set\n",
            "x --> punct --> comprising\n",
            "grayscale --> compound --> image\n",
            "image --> compound --> product\n",
            "fashion --> compound --> product\n",
            "product --> compound --> category\n",
            "category --> compound --> image\n",
            "image --> dobj --> comprising\n",
            "per --> prep --> image\n",
            "category --> compound --> training\n",
            "training --> nmod --> test\n",
            "set --> amod --> test\n",
            "image --> compound --> test\n",
            "test --> compound --> set\n",
            "set --> ROOT --> set\n",
            "image --> compound --> fashionmnist\n",
            "fashionmnist --> dobj --> set\n",
            "intended --> acl --> fashionmnist\n",
            "serve --> conj --> set\n",
            "direct --> amod --> machine\n",
            "dropin --> nmod --> replacement\n",
            "replacement --> nmod --> machine\n",
            "original --> amod --> mnist\n",
            "mnist --> nmod --> machine\n",
            "dataset --> amod --> machine\n",
            "benchmarking --> amod --> machine\n",
            "machine --> dobj --> serve\n",
            "learning --> advcl --> set\n",
            "algorithm --> compound --> share\n",
            "share --> compound --> testing\n",
            "image --> compound --> size\n",
            "size --> compound --> format\n",
            "data --> compound --> format\n",
            "format --> compound --> structure\n",
            "structure --> compound --> testing\n",
            "training --> compound --> testing\n",
            "testing --> dobj --> learning\n",
            "split --> advcl --> set\n",
            "dataset --> dobj --> split\n",
            "freely --> advmod --> available\n",
            "available --> amod --> url\n",
            "http --> compound --> url\n",
            "url --> npadvmod --> set\n",
            "\n",
            "Example Sentence: present fashionmnist new dataset comprising x grayscale image fashion product category image per category training set image test set image fashionmnist intended serve direct dropin replacement original mnist dataset benchmarking machine learning algorithm share image size data format structure training testing split dataset freely available http url\n",
            "Explanation:\n",
            "- Dependency tree shows how each word relates grammatically to others.\n",
            "- Constituency tree (not shown here) breaks the sentence into nested phrases like NP, VP, etc.\n",
            "\n",
            "‚úÖ Named Entity Recognition Counts:\n",
            "QUANTITY: 1\n",
            "CARDINAL: 1\n",
            "PRODUCT: 2\n",
            "DATE: 1\n",
            "ORDINAL: 1\n",
            "\n",
            "üéâ Entities saved to entities_sample.csv for review.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "df = pd.read_csv(\"papers_10000_cleaned.csv\")\n",
        "\n",
        "text_col = \"clean_lemmatized\"\n",
        "texts = df[text_col].dropna().astype(str).tolist()\n",
        "\n",
        "sample_texts = texts[:5]\n",
        "\n",
        "# (1) Parts of Speech (POS) Tagging\n",
        "\n",
        "pos_counts = Counter()\n",
        "\n",
        "for doc in nlp.pipe(sample_texts):\n",
        "    for token in doc:\n",
        "        if token.pos_ in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]:\n",
        "            pos_counts[token.pos_] += 1\n",
        "\n",
        "print(\"‚úÖ POS Tagging Counts:\")\n",
        "print(f\"Nouns: {pos_counts['NOUN']}\")\n",
        "print(f\"Verbs: {pos_counts['VERB']}\")\n",
        "print(f\"Adjectives: {pos_counts['ADJ']}\")\n",
        "print(f\"Adverbs: {pos_counts['ADV']}\\n\")\n",
        "\n",
        "\n",
        "# (2) Constituency & Dependency Parsing\n",
        "\n",
        "\n",
        "example_sentence = sample_texts[0]  # pick the first cleaned sentence\n",
        "doc = nlp(example_sentence)\n",
        "\n",
        "print(\"‚úÖ Dependency Parsing (word ‚Üí head relation):\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text} --> {token.dep_} --> {token.head.text}\")\n",
        "\n",
        "\n",
        "print(\"\\nExample Sentence:\", example_sentence)\n",
        "print(\"Explanation:\")\n",
        "print(\"- Dependency tree shows how each word relates grammatically to others.\")\n",
        "print(\"- Constituency tree (not shown here) breaks the sentence into nested phrases like NP, VP, etc.\")\n",
        "\n",
        "\n",
        "# (3) Named Entity Recognition (NER)\n",
        "\n",
        "entity_counts = Counter()\n",
        "\n",
        "for doc in nlp.pipe(sample_texts):\n",
        "    for ent in doc.ents:\n",
        "        entity_counts[ent.label_] += 1\n",
        "\n",
        "print(\"\\n‚úÖ Named Entity Recognition Counts:\")\n",
        "for ent, count in entity_counts.items():\n",
        "    print(f\"{ent}: {count}\")\n",
        "\n",
        "entities_table = []\n",
        "for doc in nlp.pipe(sample_texts):\n",
        "    for ent in doc.ents:\n",
        "        entities_table.append([ent.text, ent.label_])\n",
        "\n",
        "entities_df = pd.DataFrame(entities_table, columns=[\"Entity\", \"Label\"])\n",
        "entities_df.to_csv(\"entities_sample.csv\", index=False)\n",
        "print(\"\\nüéâ Entities saved to entities_sample.csv for review.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ],
      "metadata": {
        "id": "EcVqy1yj3wja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4 (20 points)."
      ],
      "metadata": {
        "id": "kEdcyHX8VaDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub‚Äôs usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ],
      "metadata": {
        "id": "1Ung5_YW3C6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ],
      "metadata": {
        "id": "CTOfUpatronW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Parameters\n",
        "base_url = \"https://github.com/marketplace?type=actions&page=\"\n",
        "total_pages = 45   # üîπ change to 50 (for ~1000 products, 20 per page)\n",
        "delay = 2         # seconds delay to avoid server overload\n",
        "\n",
        "data = []\n",
        "\n",
        "for page in range(1, total_pages + 1):\n",
        "    print(f\"Scraping page {page}...\")\n",
        "    url = base_url + str(page)\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "        if response.status_code != 200:\n",
        "            print(f\"‚ö†Ô∏è Page {page} failed with status {response.status_code}\")\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "\n",
        "        products = soup.find_all(\"div\", {\"class\": \"d-flex flex-auto\"})\n",
        "\n",
        "        for product in products:\n",
        "\n",
        "            name_tag = product.find(\"h3\")\n",
        "            name = name_tag.get_text(strip=True) if name_tag else \"N/A\"\n",
        "\n",
        "\n",
        "            desc_tag = product.find(\"p\")\n",
        "            description = desc_tag.get_text(strip=True) if desc_tag else \"N/A\"\n",
        "\n",
        "\n",
        "            link_tag = product.find(\"a\", href=True)\n",
        "            url = \"https://github.com\" + link_tag[\"href\"] if link_tag else \"N/A\"\n",
        "\n",
        "            data.append([name, description, url, page])\n",
        "\n",
        "        time.sleep(delay)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error on page {page}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(data, columns=[\"Product_Name\", \"Description\", \"URL\", \"Page_Number\"])\n",
        "df.to_csv(\"github_marketplace_actions.csv\", index=False)\n",
        "print(\"‚úÖ Scraping is complete. Data saved to github_marketplace_actions.csv\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4dtco9K--ks6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8673f40f-901b-4946-f753-73d7a36403b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping page 4...\n",
            "Scraping page 5...\n",
            "Scraping page 6...\n",
            "Scraping page 7...\n",
            "Scraping page 8...\n",
            "Scraping page 9...\n",
            "Scraping page 10...\n",
            "Scraping page 11...\n",
            "Scraping page 12...\n",
            "Scraping page 13...\n",
            "Scraping page 14...\n",
            "Scraping page 15...\n",
            "Scraping page 16...\n",
            "Scraping page 17...\n",
            "Scraping page 18...\n",
            "Scraping page 19...\n",
            "Scraping page 20...\n",
            "Scraping page 21...\n",
            "Scraping page 22...\n",
            "Scraping page 23...\n",
            "Scraping page 24...\n",
            "Scraping page 25...\n",
            "Scraping page 26...\n",
            "Scraping page 27...\n",
            "Scraping page 28...\n",
            "Scraping page 29...\n",
            "Scraping page 30...\n",
            "Scraping page 31...\n",
            "Scraping page 32...\n",
            "Scraping page 33...\n",
            "Scraping page 34...\n",
            "Scraping page 35...\n",
            "Scraping page 36...\n",
            "Scraping page 37...\n",
            "Scraping page 38...\n",
            "Scraping page 39...\n",
            "Scraping page 40...\n",
            "Scraping page 41...\n",
            "Scraping page 42...\n",
            "Scraping page 43...\n",
            "Scraping page 44...\n",
            "Scraping page 45...\n",
            "‚úÖ Scraping is complete. Data saved to github_marketplace_actions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources (first run only)\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"github_marketplace_actions.csv\")\n",
        "\n",
        "df[\"Description\"] = df[\"Description\"].fillna(\"\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"<.*?>\", \" \", text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df[\"Clean_Description\"] = df[\"Description\"].apply(clean_text)\n",
        "\n",
        "\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "df = df[df[\"Product_Name\"].notna() & df[\"URL\"].notna()]\n",
        "\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "df.to_csv(\"github_marketplace_actions_cleaned.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Preprocessing & Data Quality checks completed and Saved as github_marketplace_actions_cleaned.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ku2uQNlWKN0W",
        "outputId": "5a36f12b-0aa4-4e77-bf4c-d252387fc762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Preprocessing & Data Quality checks completed and Saved as github_marketplace_actions_cleaned.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ],
      "metadata": {
        "id": "3WeD70ty3Gui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 1\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "api_key = \"ANN78bI7tNdFOOXgY6j0pKlPe\"\n",
        "api_key_secret = \"XIBb70nEZaXsJAEQL88xSoPoYfBRwD4kMj5x8OkWgPReiIm6VG\"\n",
        "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAC4u4wEAAAAAEbXsSj%2BcPQ54yR%2BQyGReRuHqFT4%3DrAdfKwM4OzucUQf318v5epc7l434b0H7x0qmyJAxtaQGVqRKLT\"\n",
        "access_token = \"1370273475038834691-4tPW2A9BuCCXSfMTc8BpZCbZksCiFE\"\n",
        "access_token_secret = \"Cd8aAXl5kST8o4iqOEhj0J0Y050Msq4EdbZS5GTbz93P2\"\n",
        "\n",
        "client = tweepy.Client(bearer_token=bearer_token,\n",
        "                       consumer_key=api_key,\n",
        "                       consumer_secret=api_key_secret,\n",
        "                       access_token=access_token,\n",
        "                       access_token_secret=access_token_secret)\n",
        "\n",
        "\n",
        "query = \"(#AI OR #machinelearning OR #ML OR #artificialintelligence) -is:retweet lang:en\"\n",
        "\n",
        "\n",
        "print(\"üîé Searching tweets...\")\n",
        "\n",
        "response = client.search_recent_tweets(\n",
        "    query=query,\n",
        "    tweet_fields=[\"id\", \"text\", \"author_id\"],\n",
        "    user_fields=[\"username\"],\n",
        "    expansions=[\"author_id\"],\n",
        "    max_results=100\n",
        ")\n",
        "\n",
        "tweets_data = []\n",
        "\n",
        "if response.data:\n",
        "    users = {u.id: u.username for u in response.includes[\"users\"]}\n",
        "    for tweet in response.data:\n",
        "        tweets_data.append({\n",
        "            \"tweet_id\": tweet.id,\n",
        "            \"username\": users.get(tweet.author_id, \"N/A\"),\n",
        "            \"text\": tweet.text\n",
        "        })\n",
        "    print(f\"‚úÖ Found {len(tweets_data)} tweets\")\n",
        "else:\n",
        "    print(\" No tweets found.\")\n",
        "\n",
        "\n",
        "df = pd.DataFrame(tweets_data)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "mbpTDS96T7na",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69e75ee-a1ef-459e-c0b7-200fc2cfb31b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Searching tweets...\n",
            "‚úÖ Found 100 tweets\n",
            "              tweet_id      username  \\\n",
            "0  1977551287547027555   KanzaKhan09   \n",
            "1  1977551268869464297  hemettante14   \n",
            "2  1977551265212305579    raonsecure   \n",
            "3  1977551183754432683   tomas_corza   \n",
            "4  1977551121875963930       ARBSOAI   \n",
            "\n",
            "                                                text  \n",
            "0  @Tesla Wild move by FSD 14.1 backing out like ...  \n",
            "1  Talus Labs is designing intelligent systems th...  \n",
            "2  üì¢RaonSecure‚Äôs 2025 Blockchain &amp; AI Hackath...  \n",
            "3  Click the link to learn how to make your conte...  \n",
            "4  üì£ \"How to Lose at Poker While Still Claiming I...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Part 2\n",
        "\n",
        "df.drop_duplicates(subset=[\"tweet_id\"], inplace=True)\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(\"‚úÖ Final Dataset Shape:\", df.shape)\n",
        "print(\"\\nColumn Info:\")\n",
        "print(df.info())\n",
        "print(\"\\nSample Data:\")\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "df.to_csv(\"cleaned_tweets.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"üìÅ Data saved to 'cleaned_tweets.csv'\")\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download(\"cleaned_tweets.csv\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "EE4QCedK0UO4",
        "outputId": "ab422853-eb62-425b-8b0f-26dd7ba9b109"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Final Dataset Shape: (100, 3)\n",
            "\n",
            "Column Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   tweet_id  100 non-null    int64 \n",
            " 1   username  100 non-null    object\n",
            " 2   text      100 non-null    object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 2.5+ KB\n",
            "None\n",
            "\n",
            "Sample Data:\n",
            "              tweet_id      username  \\\n",
            "0  1977551287547027555   KanzaKhan09   \n",
            "1  1977551268869464297  hemettante14   \n",
            "2  1977551265212305579    raonsecure   \n",
            "3  1977551183754432683   tomas_corza   \n",
            "4  1977551121875963930       ARBSOAI   \n",
            "\n",
            "                                                text  \n",
            "0  @Tesla Wild move by FSD 14.1 backing out like ...  \n",
            "1  Talus Labs is designing intelligent systems th...  \n",
            "2  üì¢RaonSecure‚Äôs 2025 Blockchain &amp; AI Hackath...  \n",
            "3  Click the link to learn how to make your conte...  \n",
            "4  üì£ \"How to Lose at Poker While Still Claiming I...  \n",
            "üìÅ Data saved to 'cleaned_tweets.csv'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_52bf2fb2-b9b9-4b95-abd8-7db63537eed6\", \"cleaned_tweets.csv\", 24905)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment.\n",
        "\n",
        "ANS- The assignment was really interesting and useful, Since I was able to assemble real data, prepare it, and then analyze it. Although large dataset and API boundaries were the most difficult aspect to handle, I loved watching clean lessons and organized data designed for analysis."
      ],
      "metadata": {
        "id": "GiCJFJnyL6AJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ],
      "metadata": {
        "id": "JbTa-jDS-KFI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}